{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv(\"LW_Manual_scores_for_ICC_2020-05-20.csv\")\n",
    "\n",
    "#hit scores\n",
    "hit_df = scores_df[[\"subject\",\"date\",\"crossing number\",\"run\",\"limb\",\"avg_human_hit\"]]\n",
    "\n",
    "#miss\n",
    "miss_df = scores_df[[\"subject\",\"date\",\"crossing number\",\"run\",\"limb\",\"avg_human_miss\"]]\n",
    "\n",
    "#steps\n",
    "step_df = scores_df[[\"subject\",\"date\",\"crossing number\",\"run\",\"limb\",\"avg_human_steps\"]]\n",
    "\n",
    "\n",
    "\n",
    "#score extraction\n",
    "dom_f_hit = hit_df.loc[hit_df[\"limb\"]==\"Dominant Front\"]\n",
    "dom_f_miss = miss_df.loc[miss_df[\"limb\"]==\"Dominant Front\"]\n",
    "dom_f_step = step_df.loc[step_df[\"limb\"]==\"Dominant Front\"]\n",
    "\n",
    "ndom_f_hit = hit_df.loc[hit_df[\"limb\"]==\"Nondominant Front\"]\n",
    "ndom_f_miss = miss_df.loc[miss_df[\"limb\"]==\"Nondominant Front\"]\n",
    "ndom_f_step = step_df.loc[step_df[\"limb\"]==\"Nondominant Front\"]\n",
    "\n",
    "dom_b_hit = hit_df.loc[hit_df[\"limb\"]==\"Dominant Back\"]\n",
    "dom_b_miss = miss_df.loc[miss_df[\"limb\"]==\"Dominant Back\"]\n",
    "dom_b_step = step_df.loc[step_df[\"limb\"]==\"Dominant Back\"]\n",
    "\n",
    "ndom_b_hit = hit_df.loc[hit_df[\"limb\"]==\"Nondominant Back\"]\n",
    "ndom_b_miss = miss_df.loc[miss_df[\"limb\"]==\"Nondominant Back\"]\n",
    "ndom_b_step = step_df.loc[step_df[\"limb\"]==\"Nondominant Back\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_folder = glob.glob(\"*/dlc_output_resnet50/*.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC70 2019-04-11 R5 pt2\n",
      "MC70 2019-03-14 L4 pt2\n",
      "MC70 2019-04-09 L4 pt2\n"
     ]
    }
   ],
   "source": [
    "dom_f_hit_x = []\n",
    "dom_f_miss_x = []\n",
    "dom_f_step_x =[]\n",
    "\n",
    "ndom_f_hit_x = []\n",
    "ndom_f_miss_x = []\n",
    "ndom_f_step_x = []\n",
    "\n",
    "dom_b_hit_x = []\n",
    "dom_b_miss_x = []\n",
    "dom_b_step_x = []\n",
    "\n",
    "ndom_b_hit_x = []\n",
    "ndom_b_miss_x = []\n",
    "ndom_b_step_x = []\n",
    "\n",
    "#sets of labels\n",
    "dom_f_hit_y = []\n",
    "dom_f_miss_y = []\n",
    "dom_f_step_y =[]\n",
    "\n",
    "ndom_f_hit_y = []\n",
    "ndom_f_miss_y = []\n",
    "ndom_f_step_y = []\n",
    "\n",
    "dom_b_hit_y = []\n",
    "dom_b_miss_y = []\n",
    "dom_b_step_y = []\n",
    "\n",
    "ndom_b_hit_y = []\n",
    "ndom_b_miss_y = []\n",
    "ndom_b_step_y = []\n",
    "\n",
    "lengths = []\n",
    "for file in rat_folder:\n",
    "    #rat tracking file\n",
    "    #name of the rung file\n",
    "    rung_name_list = file.split(\"/\")[-1].split(\"_\")[0:8]+[\"LadderWalkMar12shuffle1\",\"450000.h5\"]\n",
    "    rung_file = '_'.join(rung_name_list)\n",
    "    #open the file\n",
    "    rat_df = pd.read_hdf(file)[\"DLC_resnet50_LadderWalkFeb13shuffle1_450000\"]\n",
    "    #properties of the file\n",
    "    subject = file.split(\"/\")[0]\n",
    "    date_raw = rung_name_list[1]\n",
    "    date = parser.parse(date_raw).date().strftime(\"%Y-%m-%d\")\n",
    "    run = rung_name_list[2]\n",
    "    crossing = [int(s) for s in rung_name_list[3] if s.isdigit()][0]\n",
    "    #\n",
    "    dom_f_hit_score = dom_f_hit[(dom_f_hit[\"subject\"]==subject) & (dom_f_hit[\"date\"]==date) & (dom_f_hit[\"run\"]==run)].reset_index()\n",
    "    dom_f_miss_score = dom_f_miss[(dom_f_miss[\"subject\"]==subject) & (dom_f_miss[\"date\"]==date) & (dom_f_miss[\"run\"]==run)].reset_index()\n",
    "    dom_f_step_score = dom_f_step[(dom_f_step[\"subject\"]==subject) & (dom_f_step[\"date\"]==date) & (dom_f_step[\"run\"]==run)].reset_index()\n",
    "\n",
    "    ndom_f_hit_score = ndom_f_hit[(ndom_f_hit[\"subject\"]==subject) & (ndom_f_hit[\"date\"]==date) & (ndom_f_hit[\"run\"]==run)].reset_index()\n",
    "    ndom_f_miss_score = ndom_f_miss[(ndom_f_miss[\"subject\"]==subject) & (ndom_f_miss[\"date\"]==date) & (ndom_f_miss[\"run\"]==run)].reset_index()\n",
    "    ndom_f_step_score = ndom_f_step[(ndom_f_step[\"subject\"]==subject) & (ndom_f_step[\"date\"]==date) & (ndom_f_step[\"run\"]==run)].reset_index()\n",
    "\n",
    "    dom_b_hit_score = dom_b_hit[(dom_b_hit[\"subject\"]==subject) & (dom_b_hit[\"date\"]==date) & (dom_b_hit[\"run\"]==run)].reset_index()\n",
    "    dom_b_miss_score = dom_b_miss[(dom_b_miss[\"subject\"]==subject) & (dom_b_miss[\"date\"]==date) & (dom_b_miss[\"run\"]==run)].reset_index()\n",
    "    dom_b_step_score = dom_b_step[(dom_b_step[\"subject\"]==subject) & (dom_b_step[\"date\"]==date) & (dom_b_step[\"run\"]==run)].reset_index()\n",
    "\n",
    "    ndom_b_hit_score = ndom_b_hit[(ndom_b_hit[\"subject\"]==subject) & (ndom_b_hit[\"date\"]==date) & (ndom_b_hit[\"run\"]==run)].reset_index()\n",
    "    ndom_b_miss_score = ndom_b_miss[(ndom_b_miss[\"subject\"]==subject) & (ndom_b_miss[\"date\"]==date) & (ndom_b_miss[\"run\"]==run)].reset_index()\n",
    "    ndom_b_step_score = ndom_b_step[(ndom_b_step[\"subject\"]==subject) & (ndom_b_step[\"date\"]==date) & (ndom_b_step[\"run\"]==run)].reset_index()\n",
    "\n",
    "    #join the rat and rung dataframes\n",
    "    df = rat_df\n",
    "    df_cols = df.columns.tolist()\n",
    "    df_temp = df\n",
    "    \n",
    "    #remove where median likelihood is low\n",
    "    df_low_like = df.columns[-df_temp[df_temp.columns.get_level_values(0).unique()].median().ge(0.2)].get_level_values(0).tolist()\n",
    "    df_cols_up = [x for x in df_cols if x not in df_low_like ]\n",
    "\n",
    "    df = rat_df[df_cols_up]\n",
    "    if df.shape[1] == 0:\n",
    "        continue\n",
    "    #if len(dom_f_low_like) >0:\n",
    "        #print(dom_f_low_like)\n",
    "    df = df.drop('likelihood', axis=1, level=1)\n",
    "\n",
    "    #scale data\n",
    "    ####Uses the values function of pandas - converts any dataframe to an array\n",
    "    data_for_scaling = df.values\n",
    "    #### the scaling object\n",
    "    scaler = MinMaxScaler()\n",
    "    #We will use fit_transform here - we want to actually scale this data, not use the scaler on a different dataset\n",
    "    scaled_data = scaler.fit_transform(data_for_scaling)\n",
    "    \n",
    "    if len(scaled_data) < 356:\n",
    "        newlength = (356-len(scaled_data))\n",
    "        zero = np.zeros((newlength,6))\n",
    "        scaled_temp = pd.DataFrame(scaled_data).append(pd.DataFrame(zero),ignore_index=True)\n",
    "        scaled_temp = scaled_temp.fillna(0)\n",
    "        scaled_data2 = scaled_temp.values\n",
    "    else:\n",
    "        scaled_data2 = scaled_data\n",
    "    \n",
    "    video_data = scaled_data2.flatten()\n",
    "    #lengths.append([len(video_data)])\n",
    "    if len(dom_f_hit_score) !=0:\n",
    "        dom_f_hit_y.append(dom_f_hit_score[\"avg_human_hit\"][0])\n",
    "        dom_f_hit_x.append(video_data)\n",
    "    if len(dom_f_miss_score) != 0:\n",
    "        dom_f_miss_y.append(dom_f_miss_score[\"avg_human_miss\"][0])\n",
    "        dom_f_miss_x.append(video_data)\n",
    "    if len(dom_f_step_score) != 0:\n",
    "        dom_f_step_y.append(dom_f_step_score[\"avg_human_steps\"][0])\n",
    "        dom_f_step_x.append(video_data)\n",
    "    if len(ndom_f_hit_score) != 0:\n",
    "        ndom_f_hit_y.append(ndom_f_hit_score[\"avg_human_hit\"][0])\n",
    "        ndom_f_hit_x.append(video_data)\n",
    "    if len(ndom_f_miss_score) != 0:\n",
    "        ndom_f_miss_y.append(ndom_f_miss_score[\"avg_human_miss\"][0])\n",
    "        ndom_f_miss_x.append(video_data)\n",
    "    if len(ndom_f_step_score) != 0:\n",
    "        ndom_f_step_y.append(ndom_f_step_score[\"avg_human_steps\"][0])\n",
    "        ndom_f_step_x.append(video_data)\n",
    "    if len(dom_b_hit_score) != 0:\n",
    "        dom_b_hit_y.append(dom_b_hit_score[\"avg_human_hit\"][0])\n",
    "        dom_b_hit_x.append(video_data)\n",
    "    if len(dom_b_miss_score) != 0:\n",
    "        dom_b_miss_y.append(dom_b_miss_score[\"avg_human_miss\"][0])\n",
    "        dom_b_miss_x.append(video_data)\n",
    "    if len(dom_b_step_score) != 0:\n",
    "        dom_b_step_y.append(dom_b_step_score[\"avg_human_steps\"][0])\n",
    "        dom_b_step_x.append(video_data)\n",
    "    if len(ndom_b_hit_score) != 0:\n",
    "        ndom_b_hit_y.append(ndom_b_hit_score[\"avg_human_hit\"][0])\n",
    "        ndom_b_hit_x.append(video_data)\n",
    "    if len(ndom_b_miss_score) != 0:\n",
    "        ndom_b_miss_y.append(ndom_b_miss_score[\"avg_human_miss\"][0])\n",
    "        ndom_b_miss_x.append(video_data)\n",
    "    if len(ndom_b_step_score) != 0:\n",
    "        ndom_b_step_y.append(ndom_b_step_score[\"avg_human_steps\"][0])\n",
    "        ndom_b_step_x.append(video_data)\n",
    "    else:\n",
    "        print(subject + \" \" + date + \" \" + run + \" pt2\")\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232 232 232 232 232 232\n",
      "231 231 231 231 231 231\n",
      "231 231 231 231 231 231\n",
      "231 231 231 231 231 231\n"
     ]
    }
   ],
   "source": [
    "print(len(dom_f_hit_x ),len(dom_f_hit_y),\n",
    "      len(dom_f_miss_x),len(dom_f_miss_y),\n",
    "      len(dom_f_step_x),len(dom_f_step_y))\n",
    "\n",
    "print(len(ndom_f_hit_x ),len(ndom_f_hit_y),\n",
    "len(ndom_f_miss_x ),len(ndom_f_miss_y ),\n",
    "len(ndom_f_step_x ),len(ndom_f_step_y))\n",
    "\n",
    "print(len(dom_b_hit_x),len(dom_b_hit_y),\n",
    "len(dom_b_miss_x ),len(dom_b_miss_y),\n",
    "len(dom_b_step_x ),len(dom_b_step_y))\n",
    "\n",
    "print(len(ndom_b_hit_x) ,len(ndom_b_hit_y),\n",
    "len(ndom_b_miss_x),len(ndom_b_miss_y),\n",
    "len(ndom_b_step_x),len(ndom_b_step_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split# Split the data into training and testing sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dom front hit\n",
    "data = np.array(dom_f_hit_x)\n",
    "labels = np.array(dom_f_hit_y)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(data, labels, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (174, 11392)\n",
      "Training Labels Shape: (174,)\n",
      "Testing Features Shape: (58, 11392)\n",
      "Testing Labels Shape: (58,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=1000, random_state=42)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model with n decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)# Train the model on training data\n",
    "rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.65 degrees.\n"
     ]
    }
   ],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.3 %.\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / test_labels)# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pickle\n",
    "pickle.dump(dom_f_hit_x,open(\"data.pkl\",\"wb\"))\n",
    "pickle.dump(dom_f_hit_y,open(\"labels.pkl\",\"wb\"))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
