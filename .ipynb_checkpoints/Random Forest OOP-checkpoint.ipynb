{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Ladder Walk Videos\n",
    "\n",
    "BIOF 509 Spring 2021\n",
    "\n",
    "This notebook contains all the code needed to train and run machine learning models to predict the different scores of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from dateutil import parser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The function separates the scores for the different limbs and processes the raw output from DeepLabCut. For each video, all features are normalized, frames are made the same length, and flattend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processing:\n",
    "    def __init__(self):\n",
    "        #list with a string order of the output of processing\n",
    "        self.keys = [\"dominant_front_hit\", \"dominant_front_miss\", \"dominant_front_step\", \n",
    " \"nondominant_front_hit\", \"nondominant_front_miss\", \"nondominant_front_step\",\n",
    "    \"dominant_back_hit\", \"dominant_back_miss\", \"dominant_back_step\", \n",
    " \"nondominant_back_hit\", \"nondominant_back_miss\", \"nondominant_back_step\"]\n",
    "    def process_with_labels(data_path,label_path,project_name,rung_network,rung_it,show_skipped):\n",
    "        scores_df = pd.read_csv(label_path)\n",
    "\n",
    "        #hit scores\n",
    "        hit_df = scores_df[[\"subject\",\"date\",\"crossing number\",\"run\",\"limb\",\"avg_human_hit\"]]\n",
    "\n",
    "        #miss\n",
    "        miss_df = scores_df[[\"subject\",\"date\",\"crossing number\",\"run\",\"limb\",\"avg_human_miss\"]]\n",
    "\n",
    "        #steps\n",
    "        step_df = scores_df[[\"subject\",\"date\",\"crossing number\",\"run\",\"limb\",\"avg_human_steps\"]]\n",
    "\n",
    "        #score extraction\n",
    "        dom_f_hit = hit_df.loc[hit_df[\"limb\"]==\"Dominant Front\"]\n",
    "        dom_f_miss = miss_df.loc[miss_df[\"limb\"]==\"Dominant Front\"]\n",
    "        dom_f_step = step_df.loc[step_df[\"limb\"]==\"Dominant Front\"]\n",
    "\n",
    "        ndom_f_hit = hit_df.loc[hit_df[\"limb\"]==\"Nondominant Front\"]\n",
    "        ndom_f_miss = miss_df.loc[miss_df[\"limb\"]==\"Nondominant Front\"]\n",
    "        ndom_f_step = step_df.loc[step_df[\"limb\"]==\"Nondominant Front\"]\n",
    "\n",
    "        dom_b_hit = hit_df.loc[hit_df[\"limb\"]==\"Dominant Back\"]\n",
    "        dom_b_miss = miss_df.loc[miss_df[\"limb\"]==\"Dominant Back\"]\n",
    "        dom_b_step = step_df.loc[step_df[\"limb\"]==\"Dominant Back\"]\n",
    "\n",
    "        ndom_b_hit = hit_df.loc[hit_df[\"limb\"]==\"Nondominant Back\"]\n",
    "        ndom_b_miss = miss_df.loc[miss_df[\"limb\"]==\"Nondominant Back\"]\n",
    "        ndom_b_step = step_df.loc[step_df[\"limb\"]==\"Nondominant Back\"]\n",
    "        dom_f_hit_x = []\n",
    "        dom_f_miss_x = []\n",
    "        dom_f_step_x =[]\n",
    "\n",
    "        ndom_f_hit_x = []\n",
    "        ndom_f_miss_x = []\n",
    "        ndom_f_step_x = []\n",
    "\n",
    "        dom_b_hit_x = []\n",
    "        dom_b_miss_x = []\n",
    "        dom_b_step_x = []\n",
    "\n",
    "        ndom_b_hit_x = []\n",
    "        ndom_b_miss_x = []\n",
    "        ndom_b_step_x = []\n",
    "\n",
    "        #sets of labels\n",
    "        dom_f_hit_y = []\n",
    "        dom_f_miss_y = []\n",
    "        dom_f_step_y =[]\n",
    "\n",
    "        ndom_f_hit_y = []\n",
    "        ndom_f_miss_y = []\n",
    "        ndom_f_step_y = []\n",
    "\n",
    "        dom_b_hit_y = []\n",
    "        dom_b_miss_y = []\n",
    "        dom_b_step_y = []\n",
    "\n",
    "        ndom_b_hit_y = []\n",
    "        ndom_b_miss_y = []\n",
    "        ndom_b_step_y = []\n",
    "\n",
    "        lengths = []\n",
    "        \n",
    "        #video_list\n",
    "        dom_f_hit_videos = []\n",
    "        dom_f_miss_videos = []\n",
    "        dom_f_step_videos =[]\n",
    "\n",
    "        ndom_f_hit_videos = []\n",
    "        ndom_f_miss_videos = []\n",
    "        ndom_f_step_videos = []\n",
    "\n",
    "        dom_b_hit_videos = []\n",
    "        dom_b_miss_videos = []\n",
    "        dom_b_step_videos = []\n",
    "\n",
    "        ndom_b_hit_videos = []\n",
    "        ndom_b_miss_videos = []\n",
    "        ndom_b_step_videos = []\n",
    "        \n",
    "        rat_folder = glob.glob(data_path)\n",
    "        \n",
    "        for file in rat_folder:\n",
    "            #rung file info\n",
    "            rung_name_list = file.split(\"/\")[-1].split(\"_\")[0:8]+[rung_network,str(rung_it)+\".h5\"]\n",
    "            rung_file = '_'.join(rung_name_list)\n",
    "            #rat tracking file\n",
    "            #open the file\n",
    "            rat_df = pd.read_hdf(file)[project_name]\n",
    "            #properties of the file\n",
    "            subject = file.split(\"/\")[0]\n",
    "            date_raw = rung_name_list[1]\n",
    "            date = parser.parse(date_raw).date().strftime(\"%Y-%m-%d\")\n",
    "            run = rung_name_list[2]\n",
    "            crossing = [int(s) for s in rung_name_list[3] if s.isdigit()][0]\n",
    "                \n",
    "            \n",
    "            #\n",
    "            dom_f_hit_score = dom_f_hit[(dom_f_hit[\"subject\"]==subject) & (dom_f_hit[\"date\"]==date) & (dom_f_hit[\"run\"]==run)].reset_index()\n",
    "            dom_f_miss_score = dom_f_miss[(dom_f_miss[\"subject\"]==subject) & (dom_f_miss[\"date\"]==date) & (dom_f_miss[\"run\"]==run)].reset_index()\n",
    "            dom_f_step_score = dom_f_step[(dom_f_step[\"subject\"]==subject) & (dom_f_step[\"date\"]==date) & (dom_f_step[\"run\"]==run)].reset_index()\n",
    "\n",
    "            ndom_f_hit_score = ndom_f_hit[(ndom_f_hit[\"subject\"]==subject) & (ndom_f_hit[\"date\"]==date) & (ndom_f_hit[\"run\"]==run)].reset_index()\n",
    "            ndom_f_miss_score = ndom_f_miss[(ndom_f_miss[\"subject\"]==subject) & (ndom_f_miss[\"date\"]==date) & (ndom_f_miss[\"run\"]==run)].reset_index()\n",
    "            ndom_f_step_score = ndom_f_step[(ndom_f_step[\"subject\"]==subject) & (ndom_f_step[\"date\"]==date) & (ndom_f_step[\"run\"]==run)].reset_index()\n",
    "\n",
    "            dom_b_hit_score = dom_b_hit[(dom_b_hit[\"subject\"]==subject) & (dom_b_hit[\"date\"]==date) & (dom_b_hit[\"run\"]==run)].reset_index()\n",
    "            dom_b_miss_score = dom_b_miss[(dom_b_miss[\"subject\"]==subject) & (dom_b_miss[\"date\"]==date) & (dom_b_miss[\"run\"]==run)].reset_index()\n",
    "            dom_b_step_score = dom_b_step[(dom_b_step[\"subject\"]==subject) & (dom_b_step[\"date\"]==date) & (dom_b_step[\"run\"]==run)].reset_index()\n",
    "\n",
    "            ndom_b_hit_score = ndom_b_hit[(ndom_b_hit[\"subject\"]==subject) & (ndom_b_hit[\"date\"]==date) & (ndom_b_hit[\"run\"]==run)].reset_index()\n",
    "            ndom_b_miss_score = ndom_b_miss[(ndom_b_miss[\"subject\"]==subject) & (ndom_b_miss[\"date\"]==date) & (ndom_b_miss[\"run\"]==run)].reset_index()\n",
    "            ndom_b_step_score = ndom_b_step[(ndom_b_step[\"subject\"]==subject) & (ndom_b_step[\"date\"]==date) & (ndom_b_step[\"run\"]==run)].reset_index()\n",
    "\n",
    "            \n",
    "            video_df = scores_df[[\"subject\",\"date\",\"crossing number\",\"run\"]]\n",
    "            video_info = video_df[(video_df[\"subject\"]==subject) & (video_df[\"date\"]==date) & (video_df[\"run\"]==run)].reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "            #join the rat and rung dataframes\n",
    "            df = rat_df\n",
    "            df_cols = df.columns.tolist()\n",
    "            df_temp = df\n",
    "\n",
    "            #remove where median likelihood is low\n",
    "            #df_low_like = df.columns[-df_temp[df_temp.columns.get_level_values(0).unique()].median().ge(0.2)].get_level_values(0).tolist()\n",
    "            #df_cols_up = [x for x in df_cols if x not in df_low_like ]\n",
    "\n",
    "            #df = rat_df[df_cols_up]\n",
    "            if df.shape[1] == 0:\n",
    "                continue\n",
    "            #if len(dom_f_low_like) >0:\n",
    "                #print(dom_f_low_like)\n",
    "            df = df.drop('likelihood', axis=1, level=1)\n",
    "\n",
    "            #scale data\n",
    "            ####Uses the values function of pandas - converts any dataframe to an array\n",
    "            data_for_scaling = df.values\n",
    "            #### the scaling object\n",
    "            scaler = MinMaxScaler()\n",
    "            #We will use fit_transform here - we want to actually scale this data, not use the scaler on a different dataset\n",
    "            scaled_data = scaler.fit_transform(data_for_scaling)\n",
    "\n",
    "            if len(scaled_data) < 356:\n",
    "                newlength = (356-len(scaled_data))\n",
    "                zero = np.zeros((newlength,6))\n",
    "                scaled_temp = pd.DataFrame(scaled_data).append(pd.DataFrame(zero),ignore_index=True)\n",
    "                scaled_temp = scaled_temp.fillna(0)\n",
    "                scaled_data2 = scaled_temp.values\n",
    "            else:\n",
    "                scaled_data2 = scaled_data\n",
    "\n",
    "            video_data = scaled_data2.flatten()\n",
    "            #lengths.append([len(video_data)])\n",
    "            if len(dom_f_hit_score) !=0:\n",
    "                dom_f_hit_y.append(dom_f_hit_score[\"avg_human_hit\"][0])\n",
    "                dom_f_hit_x.append(video_data)\n",
    "                dom_f_hit_videos.append(video_info.iloc[0])\n",
    "            else:\n",
    "                if show_skipped:\n",
    "                    print(\"Dom Front Hit\"+\"Missing scores: \"+subject + \" \" + date + \" \" + run)\n",
    "            if len(dom_f_miss_score) != 0:\n",
    "                dom_f_miss_y.append(dom_f_miss_score[\"avg_human_miss\"][0])\n",
    "                dom_f_miss_x.append(video_data)\n",
    "                dom_f_miss_videos.append(video_info.iloc[0])\n",
    "            else:\n",
    "                if show_skipped:\n",
    "                    print(\"Dom Front Miss\"+\"Missing scores: \"+subject + \" \" + date + \" \" + run)\n",
    "            if len(dom_f_step_score) != 0:\n",
    "                dom_f_step_y.append(dom_f_step_score[\"avg_human_steps\"][0])\n",
    "                dom_f_step_x.append(video_data)\n",
    "                dom_f_step_videos.append(video_info.iloc[0])\n",
    "            else:\n",
    "                if show_skipped:\n",
    "                    print(\"Dom Front Step\"+\"Missing scores: \"+subject + \" \" + date + \" \" + run)\n",
    "            if len(ndom_f_hit_score) != 0:\n",
    "                ndom_f_hit_y.append(ndom_f_hit_score[\"avg_human_hit\"][0])\n",
    "                ndom_f_hit_x.append(video_data)\n",
    "                ndom_f_hit_videos.append(video_info.iloc[0])\n",
    "            else:\n",
    "                if show_skipped:\n",
    "                    print(\"Nondom Front Hit\"+\"Missing scores: \"+subject + \" \" + date + \" \" + run)\n",
    "            if len(ndom_f_miss_score) != 0:\n",
    "                ndom_f_miss_y.append(ndom_f_miss_score[\"avg_human_miss\"][0])\n",
    "                ndom_f_miss_x.append(video_data)\n",
    "                ndom_f_miss_videos.append(video_info.iloc[0])\n",
    "            else:\n",
    "                if show_skipped:\n",
    "                    print(\"Nondom Front Miss\"+\"Missing scores: \"+subject + \" \" + date + \" \" + run)\n",
    "            if len(ndom_f_step_score) != 0:\n",
    "                ndom_f_step_y.append(ndom_f_step_score[\"avg_human_steps\"][0])\n",
    "                ndom_f_step_x.append(video_data)\n",
    "                ndom_f_step_videos.append(video_info.iloc[0])\n",
    "            else:\n",
    "                if show_skipped:\n",
    "                    print(\"Nondom Front Step\"+\"Missing scores: \"+subject + \" \" + date + \" \" + run)\n",
    "            if len(dom_b_hit_score) != 0:\n",
    "                dom_b_hit_y.append(dom_b_hit_score[\"avg_human_hit\"][0])\n",
    "                dom_b_hit_x.append(video_data)\n",
    "                dom_b_hit_videos.append(video_info.iloc[0])\n",
    "            else:\n",
    "                if show_skipped:\n",
    "                    print(\"Dom Back Hit\"+\"Missing scores: \"+subject + \" \" + date + \" \" + run)\n",
    "            if len(dom_b_miss_score) != 0:\n",
    "                dom_b_miss_y.append(dom_b_miss_score[\"avg_human_miss\"][0])\n",
    "                dom_b_miss_x.append(video_data)\n",
    "                dom_b_miss_videos.append(video_info.iloc[0])\n",
    "            else:\n",
    "                if show_skipped:\n",
    "                    print(\"Dom Back Miss\"+\"Missing scores: \"+subject + \" \" + date + \" \" + run)\n",
    "            if len(dom_b_step_score) != 0:\n",
    "                dom_b_step_y.append(dom_b_step_score[\"avg_human_steps\"][0])\n",
    "                dom_b_step_x.append(video_data)\n",
    "                dom_b_step_videos.append(video_info.iloc[0])\n",
    "            else:\n",
    "                if show_skipped:\n",
    "                    print(\"Dom Back Step\"+\"Missing scores: \"+subject + \" \" + date + \" \" + run)\n",
    "            if len(ndom_b_hit_score) != 0:\n",
    "                ndom_b_hit_y.append(ndom_b_hit_score[\"avg_human_hit\"][0])\n",
    "                ndom_b_hit_x.append(video_data)\n",
    "                ndom_b_hit_videos.append(video_info.iloc[0])\n",
    "            else:\n",
    "                if show_skipped:\n",
    "                    print(\"Nondom Back Hit\"+\"Missing scores: \"+subject + \" \" + date + \" \" + run)\n",
    "            if len(ndom_b_miss_score) != 0:\n",
    "                ndom_b_miss_y.append(ndom_b_miss_score[\"avg_human_miss\"][0])\n",
    "                ndom_b_miss_x.append(video_data)\n",
    "                ndom_b_miss_videos.append(video_info.iloc[0])\n",
    "            else:\n",
    "                if show_skipped:\n",
    "                    print(\"Nondom Back Miss\"+\"Missing scores: \"+subject + \" \" + date + \" \" + run)\n",
    "            if len(ndom_b_step_score) != 0:\n",
    "                ndom_b_step_y.append(ndom_b_step_score[\"avg_human_steps\"][0])\n",
    "                ndom_b_step_x.append(video_data)\n",
    "                ndom_b_step_videos.append(video_info.iloc[0])\n",
    "            else:\n",
    "                if show_skipped:\n",
    "                    print(\"Nondom Back Step\"+\"Missing scores: \"+subject + \" \" + date + \" \" + run)\n",
    "                continue\n",
    "        return [dom_f_hit_x, dom_f_miss_x, dom_f_step_x, ndom_f_hit_x, ndom_f_miss_x, ndom_f_step_x,\n",
    "    dom_b_hit_x, dom_b_miss_x, dom_b_step_x, ndom_b_hit_x, ndom_b_miss_x, ndom_b_step_x],[dom_f_hit_y, dom_f_miss_y, dom_f_step_y, ndom_f_hit_y, ndom_f_miss_y, ndom_f_step_y, dom_b_hit_y,\n",
    "    dom_b_miss_y, dom_b_step_y, ndom_b_hit_y, ndom_b_miss_y, ndom_b_step_y],[dom_f_hit_videos, dom_f_miss_videos, dom_f_step_videos, ndom_f_hit_videos, ndom_f_miss_videos, ndom_f_step_videos, dom_b_hit_videos,\n",
    "    dom_b_miss_videos, dom_b_step_videos, ndom_b_hit_videos, ndom_b_miss_videos, ndom_b_step_videos]\n",
    "\n",
    "    def process_data(data_path,project_name,rung_network,rung_it,max_len,show_skipped):\n",
    "        \n",
    "        X = []\n",
    "        video_list = []\n",
    "\n",
    "        rat_folder = glob.glob(data_path)\n",
    "        \n",
    "        for file in rat_folder:\n",
    "            #rung file info\n",
    "            rung_name_list = file.split(\"/\")[-1].split(\"_\")[0:8]+[rung_network,str(rung_it)+\".h5\"]\n",
    "            rung_file = '_'.join(rung_name_list)\n",
    "            #rat tracking file\n",
    "            #open the file\n",
    "            rat_df = pd.read_hdf(file)[project_name]\n",
    "            #properties of the file\n",
    "            subject = file.split(\"/\")[0]\n",
    "            date_raw = rung_name_list[1]\n",
    "            date = parser.parse(date_raw).date().strftime(\"%Y-%m-%d\")\n",
    "            run = rung_name_list[2]\n",
    "            crossing = [int(s) for s in rung_name_list[3] if s.isdigit()][0]\n",
    "                \n",
    "            video_info = pd.DataFrame({\"subject\":[subject],\"date\":[date],\"crossing\":[crossing],\"run\":[run]})\n",
    "\n",
    "            df = rat_df\n",
    "            df_cols = df.columns.tolist()\n",
    "            df_temp = df\n",
    "\n",
    "            #remove where median likelihood is low\n",
    "            #df_low_like = df.columns[-df_temp[df_temp.columns.get_level_values(0).unique()].median().ge(0.2)].get_level_values(0).tolist()\n",
    "            #df_cols_up = [x for x in df_cols if x not in df_low_like ]\n",
    "\n",
    "            #df = rat_df[df_cols_up]\n",
    "            if df.shape[1] == 0:\n",
    "                continue\n",
    "            #if len(dom_f_low_like) >0:\n",
    "                #print(dom_f_low_like)\n",
    "            df = df.drop('likelihood', axis=1, level=1)\n",
    "\n",
    "            #scale data\n",
    "            ####Uses the values function of pandas - converts any dataframe to an array\n",
    "            data_for_scaling = df.values\n",
    "            #### the scaling object\n",
    "            scaler = MinMaxScaler()\n",
    "            #We will use fit_transform here - we want to actually scale this data, not use the scaler on a different dataset\n",
    "            scaled_data = scaler.fit_transform(data_for_scaling)\n",
    "\n",
    "            if len(scaled_data) < max_len:\n",
    "                newlength = (max_len-len(scaled_data))\n",
    "                zero = np.zeros((newlength,6))\n",
    "                scaled_temp = pd.DataFrame(scaled_data).append(pd.DataFrame(zero),ignore_index=True)\n",
    "                scaled_temp = scaled_temp.fillna(0)\n",
    "                scaled_data2 = scaled_temp.values\n",
    "            else:\n",
    "                scaled_data2 = scaled_data\n",
    "\n",
    "            video_data = scaled_data2.flatten()\n",
    "            X.append(video_data)\n",
    "            video_list.append(video_info)\n",
    "            \n",
    "        return X,video_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "\n",
    "This class makes scoring possible with a support vector regressor and random forest regreesor. The result is some metrics of interest and a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scoring:\n",
    "    def support_vector(X,y,name,kern,save=True):\n",
    "        data = np.array(X)\n",
    "        labels = np.array(y)\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(data, labels, test_size = 0.2, random_state = 42)\n",
    "        regressor = SVR(kernel=kern)\n",
    "        regressor.fit(train_features, train_labels)\n",
    "        predictions = regressor.predict(test_features)# Calculate the absolute errors\n",
    "        errors = abs(predictions - test_labels)# Print out the mean absolute error (mae)\n",
    "        # Calculate mean absolute percentage error (MAPE)\n",
    "        mape = 100 * (errors / test_labels)# Calculate and display accuracy\n",
    "        rmse = mean_squared_error(test_labels,predictions,squared=True)\n",
    "        accuracy = 100 - np.mean(mape)\n",
    "        if save == True:\n",
    "            joblib.dump(regressor, name+\".joblib\", compress=0)\n",
    "        return accuracy,rmse\n",
    "    def forest(X,y,name,data_path,project_name, njobs=None,save=True):\n",
    "        data = np.array(X)\n",
    "        labels = np.array(y)\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(data, labels, test_size = 0.2, random_state = 42)\n",
    "        # Instantiate model with n decision trees\n",
    "        rf = RandomForestRegressor(n_estimators = 1000, random_state = 42,n_jobs=njobs)# Train the model on training data\n",
    "        rf.fit(train_features, train_labels)\n",
    "        # Use the forest's predict method on the test data\n",
    "        predictions = rf.predict(test_features)# Calculate the absolute errors\n",
    "        errors = abs(predictions - test_labels)# Print out the mean absolute error (mae)\n",
    "        # Calculate mean absolute percentage error (MAPE)\n",
    "        mape = 100 * (errors / test_labels)# Calculate and display accuracy\n",
    "        accuracy = 100 - np.mean(mape)\n",
    "        rmse = mean_squared_error(test_labels,predictions,squared=True)\n",
    "        \n",
    "        if save == True:\n",
    "            joblib.dump(rf, name+\".joblib\", compress=0) \n",
    "            #print(f\"Uncompressed Random Forest: {np.round(os.path.getsize(rf_name+'.joblib') / 1024 / 1024, 2) } MB\")\n",
    "        \n",
    "        rat_folder = glob.glob(data_path)\n",
    "        df = pd.read_hdf(rat_folder[0])[project_name]\n",
    "        df = df.drop('likelihood', axis=1, level=1)\n",
    "        feats = {} # a dict to hold feature_name: feature_importance\n",
    "        for feature, importance in zip(df.columns, rf.feature_importances_):\n",
    "            feats[feature] = importance #add the name/value pair \n",
    "        importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "        return predictions,errors,mape,accuracy,rmse,importances.sort_values(by=\"Gini-importance\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = Processing.process_with_labels(\"*/dlc_output_resnet50/*.h5\",\"LW_Manual_scores_for_ICC_2020-05-20.csv\",\"DLC_resnet50_LadderWalkFeb13shuffle1_450000\",\"LadderWalkMar12shuffle1\",rung_it=450000,show_skipped = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = datas[0]\n",
    "labels_y = datas[1]\n",
    "videos = datas[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and save the support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dominant_front_hit\n",
      "accuracy:  81.49738841675024  RMSE:  0.6367471567401566\n",
      "1 dominant_front_miss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-99eec254032c>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  mape = 100 * (errors / test_labels)# Calculate and display accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  -inf  RMSE:  0.851113772828861\n",
      "2 dominant_front_step\n",
      "accuracy:  87.7935220047853  RMSE:  0.7090797674513794\n",
      "3 nondominant_front_hit\n",
      "accuracy:  86.53867990781295  RMSE:  0.6345549453646989\n",
      "4 nondominant_front_miss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-99eec254032c>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  mape = 100 * (errors / test_labels)# Calculate and display accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  -inf  RMSE:  0.18857045264878267\n",
      "5 nondominant_front_step\n",
      "accuracy:  85.02256467467588  RMSE:  0.7360277948512086\n",
      "6 dominant_back_hit\n",
      "accuracy:  86.97608356360936  RMSE:  0.3196878367470701\n",
      "7 dominant_back_miss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-99eec254032c>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  mape = 100 * (errors / test_labels)# Calculate and display accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  -inf  RMSE:  0.2710642785596339\n",
      "8 dominant_back_step\n",
      "accuracy:  89.76351535316701  RMSE:  0.2539401459284037\n",
      "9 nondominant_back_hit\n",
      "accuracy:  88.0646789214716  RMSE:  0.2904149005470281\n",
      "10 nondominant_back_miss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-99eec254032c>:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  mape = 100 * (errors / test_labels)# Calculate and display accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  -inf  RMSE:  0.22521397195566492\n",
      "11 nondominant_back_step\n",
      "accuracy:  89.60608060900736  RMSE:  0.27027278691849765\n"
     ]
    }
   ],
   "source": [
    "for i,k in enumerate(Processing().keys):\n",
    "    print(i,k)\n",
    "    svr_results = Scoring.support_vector(data_x[i],labels_y[i],k+\"_svr\",kern=\"rbf\",save=True)\n",
    "    print(\"accuracy: \", svr_results[0],\" RMSE: \", svr_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and save random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dominant_front_hit\n",
      "accuracy:  82.11270034929609  RMSE:  0.6114119290780143  Top Features: \n",
      "                    Gini-importance\n",
      "(left fingers, y)         0.002786\n",
      "(right ankle, y)          0.002783\n",
      "(left wrist, y)           0.001687\n",
      "(hip, y)                  0.001400\n",
      "(left ankle, y)           0.001380\n",
      "1 dominant_front_miss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-99eec254032c>:28: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  mape = 100 * (errors / test_labels)# Calculate and display accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  -inf  RMSE:  0.769741510638298  Top Features: \n",
      "                    Gini-importance\n",
      "(right eye, y)            0.002610\n",
      "(left fingers, y)         0.002061\n",
      "(nose, y)                 0.001460\n",
      "(left wrist, y)           0.001326\n",
      "(right ankle, y)          0.000507\n",
      "2 dominant_front_step\n",
      "accuracy:  87.82283696608636  RMSE:  0.7351758203309677  Top Features: \n",
      "                    Gini-importance\n",
      "(left fingers, y)         0.001347\n",
      "(base of tail, y)         0.000822\n",
      "(left ankle, y)           0.000809\n",
      "(right elbow, y)          0.000674\n",
      "(right ankle, y)          0.000559\n",
      "3 nondominant_front_hit\n",
      "accuracy:  85.79727826375145  RMSE:  0.7382999834515391  Top Features: \n",
      "                  Gini-importance\n",
      "(hip, y)                0.000896\n",
      "(left elbow, y)         0.000519\n",
      "(right toes, y)         0.000510\n",
      "(left wrist, y)         0.000491\n",
      "(nose, y)               0.000476\n",
      "4 nondominant_front_miss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-99eec254032c>:28: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  mape = 100 * (errors / test_labels)# Calculate and display accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  -inf  RMSE:  0.25520445153664334  Top Features: \n",
      "                   Gini-importance\n",
      "(left eye, y)            0.006461\n",
      "(left toes, y)           0.001739\n",
      "(right ankle, y)         0.001710\n",
      "(nose, y)                0.001166\n",
      "(right eye, y)           0.001145\n",
      "5 nondominant_front_step\n",
      "accuracy:  85.96109723547757  RMSE:  0.7996752553191462  Top Features: \n",
      "                    Gini-importance\n",
      "(left elbow, y)           0.000896\n",
      "(hip, y)                  0.000424\n",
      "(left fingers, y)         0.000391\n",
      "(left wrist, y)           0.000244\n",
      "(right ankle, y)          0.000191\n",
      "6 dominant_back_hit\n",
      "accuracy:  86.13756133936978  RMSE:  0.3726776997635938  Top Features: \n",
      "                    Gini-importance\n",
      "(hip, y)                  0.004001\n",
      "(left elbow, y)           0.002062\n",
      "(left fingers, y)         0.001907\n",
      "(right wrist, y)          0.001287\n",
      "(right ankle, y)          0.001271\n",
      "7 dominant_back_miss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-99eec254032c>:28: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  mape = 100 * (errors / test_labels)# Calculate and display accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  -inf  RMSE:  0.28605895271867604  Top Features: \n",
      "                    Gini-importance\n",
      "(left toes, y)            0.002155\n",
      "(right wrist, y)          0.000676\n",
      "(left fingers, y)         0.000553\n",
      "(hip, y)                  0.000510\n",
      "(right ankle, y)          0.000467\n",
      "8 dominant_back_step\n",
      "accuracy:  89.81079459842756  RMSE:  0.26266828841607737  Top Features: \n",
      "                    Gini-importance\n",
      "(right ankle, y)          0.003611\n",
      "(left elbow, y)           0.002479\n",
      "(left fingers, y)         0.001531\n",
      "(right eye, y)            0.000946\n",
      "(left toes, y)            0.000882\n",
      "9 nondominant_back_hit\n",
      "accuracy:  88.17773108097578  RMSE:  0.2800198416075649  Top Features: \n",
      "                    Gini-importance\n",
      "(left ankle, y)           0.001845\n",
      "(right ankle, y)          0.001254\n",
      "(left elbow, y)           0.001226\n",
      "(nose, y)                 0.001218\n",
      "(base of tail, y)         0.000871\n",
      "10 nondominant_back_miss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-99eec254032c>:28: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  mape = 100 * (errors / test_labels)# Calculate and display accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  -inf  RMSE:  0.22126353191489354  Top Features: \n",
      "                     Gini-importance\n",
      "(left toes, y)             0.000333\n",
      "(right ankle, y)           0.000273\n",
      "(right fingers, y)         0.000218\n",
      "(left eye, x)              0.000203\n",
      "(right toes, y)            0.000150\n",
      "11 nondominant_back_step\n",
      "accuracy:  89.97073172099769  RMSE:  0.25062372340425493  Top Features: \n",
      "                   Gini-importance\n",
      "(left toes, y)           0.002809\n",
      "(right ankle, y)         0.001439\n",
      "(left elbow, y)          0.001239\n",
      "(hip, y)                 0.001229\n",
      "(left ankle, y)          0.000704\n"
     ]
    }
   ],
   "source": [
    "for i,k in enumerate(Processing().keys):\n",
    "    print(i,k)\n",
    "    forest_results = Scoring.forest(data_x[i],labels_y[i],k+\"_random_forest\",\"*/dlc_output_resnet50/*.h5\",\"DLC_resnet50_LadderWalkFeb13shuffle1_450000\",njobs=6,save=True)\n",
    "    print(\"accuracy: \", forest_results[3],\" RMSE: \", forest_results[4],\" Top Features: \\n\",forest_results[5].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation with KFold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7094229827122307, 0.7535589655288377, 0.7872374389131778, 0.7680611759984091]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "loaded_model = joblib.load(\"dominant_front_hit_random_forest.joblib\")\n",
    "\n",
    "\n",
    "temp_data = np.array(data_x[0])\n",
    "temp_labels = np.array(labels_y[0])\n",
    "temp_vids = np.array(videos[0])\n",
    "\n",
    "scores = []\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "for train_index,test_index in kf.split(temp_data):\n",
    "    X_train, X_test, y_train, y_test = temp_data[train_index], temp_data[test_index], temp_labels[train_index], temp_labels[test_index]\n",
    "    scores.append(loaded_model.score(X_test, y_test))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of using loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8000538634784186\n"
     ]
    }
   ],
   "source": [
    "loaded_model = joblib.load(\"dominant_front_hit_random_forest.joblib\")\n",
    "temp_data = np.array(data_x[0])\n",
    "temp_labels = np.array(labels_y[0])\n",
    "temp_vids = np.array(videos[0])\n",
    "\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(temp_data, temp_labels, test_size = 0.5, random_state = 1)\n",
    "\n",
    "result = loaded_model.score(test_features, test_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of use\n",
    "\n",
    "I imagine that this tool can be used to score videos and quickly output a dataframe with the video information and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    predictions\n",
      "0      4.432667\n",
      "1      4.807000\n",
      "2      3.361000\n",
      "3      4.685000\n",
      "4      4.095000\n",
      "5      3.361000\n",
      "6      3.241000\n",
      "7      3.469333\n",
      "8      4.619333\n",
      "9      4.017333\n",
      "10     3.520000\n",
      "11     3.652333\n",
      "12     3.385333\n",
      "13     3.559333\n",
      "14     3.789667\n",
      "15     3.619000\n",
      "16     3.520000\n",
      "17     2.829000\n",
      "18     4.096667\n",
      "19     3.558667\n",
      "20     3.357667\n",
      "21     2.871333\n",
      "22     2.978333\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#process data without labels\n",
    "datas = Processing.process_data(\"*/dlc_output_resnet50/*.h5\",\"DLC_resnet50_LadderWalkFeb13shuffle1_450000\",\"LadderWalkMar12shuffle1\",rung_it=450000,max_len=356,show_skipped = False)\n",
    "\n",
    "data_x = datas[0]\n",
    "videos = datas[1]\n",
    "\n",
    "temp_data = np.array(data_x)\n",
    "temp_vids = np.array(videos)\n",
    "\n",
    "\n",
    "#get a random sample of the dataset\n",
    "random.seed(23)\n",
    "random.shuffle(temp_data)\n",
    "indexes = []\n",
    "values = []\n",
    "for idx, val in random.sample(list(enumerate(temp_data)), (len(temp_data)//10)):\n",
    "    indexes.append(idx)\n",
    "    values.append(val)\n",
    "\n",
    "#get video info\n",
    "vids = temp_vids[indexes]\n",
    "\n",
    "#load model\n",
    "loaded_model = joblib.load(\"dominant_front_hit_random_forest.joblib\")\n",
    "pred_labels = loaded_model.predict(values)\n",
    "\n",
    "ex_df = pd.DataFrame(np.concatenate(vids),columns=[\"Subject\",\"Date\",\"Crossing\",\"Run\"])\n",
    "ex_df[\"predictions\"] = pred_labels\n",
    "print(ex_df[[\"predictions\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
